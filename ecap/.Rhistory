probs$p_flip <- sapply(probs$p.tilde, pFlip)
probs <- probs[order(probs$p_flip),]
## For MLE Later -- obtain the win variable
win_index <- which(probs$win_var == win_id)
lose_index <- (1:length(win_var))[-win_index]
## Generate basis function / omega matrix from p.tilde
# Get Knot Locations
probs_flip <- probs$p_flip
knot.range <- range(probs$p_flip)
quantiles <- seq(knot.range[1]+0.0001, knot.range[2]-0.0001, length = 50)
# Generate the basis matrix and its correspoding 1st and 2nd deriv's
basis_0 <- myns(probs_flip, knots = quantiles, intercept = T, Boundary.knots = knot.range)
basis_1 <- myns(probs_flip, knots = quantiles, deriv = 1, intercept = T, Boundary.knots = knot.range)
basis_2 <- myns(probs_flip, knots = quantiles, deriv = 2, intercept = T, Boundary.knots = knot.range)
basis_sum <- t(basis_0)%*%basis_0
sum_b_d1 <- t(basis_1)%*%rep(1,nrow(basis_1))
# We also want to calculate Omega on a fine grid of points
fine_grid <- seq(0, 0.5, by=0.001)
basis_fine_grid <- myns(fine_grid, knots = quantiles, intercept = T, Boundary.knots = knot.range)
basis_fine_grid_d2 <- myns(fine_grid, knots = quantiles, deriv = 2, intercept = T, Boundary.knots = knot.range)
omega <- (1/nrow(basis_fine_grid)) * (t(basis_fine_grid_d2) %*% basis_fine_grid_d2)
## Grid for the optimization algorithm
pt <- c(seq(10^(-12), 0.5, by = 0.001), 0.5)
basis_0.grid <- myns(pt, knots = quantiles, intercept = T, Boundary.knots = knot.range)
basis_1.grid <- myns(pt, knots = quantiles, deriv = 1, intercept = T, Boundary.knots = knot.range)
basis_sum.grid <- t(basis_0.grid)%*%basis_0.grid
#### Adjustment 1: Risk function for lambda and grid for gamma. True p ####
## CV SET UP to get min value of lambda from risk function
# Randomize the rows
rows <- 1:nrow(basis_0)
rows_rand <- rows[sample(rows)]
## Declate the number of groups that we want
n.group <- 10
## Return a list with 10 approx equal vectors of rows.
partitions <- split(rows_rand,
cut(rows_rand,quantile(rows_rand,(0:n.group)/n.group),
include.lowest=TRUE, labels=FALSE))
for(jjj in 1:n.group) {
partitions[[jjj]] <- rows_rand[partitions[[jjj]]]
}
### Here we are going to pick the best value of lambda through cross validation
risk_cvsplit <- lapply(1:n.group, function(j) {
return(risk_cvsplit_fcn(j, partitions, basis_0, basis_1, probs$p_flip, lambda_grid, pt, omega, basis_0.grid, basis_1.grid))
})
r_cv_split_matrix <- do.call(cbind, risk_cvsplit)
r_cv_split_vec <- apply(r_cv_split_matrix, 1, mean)
names(r_cv_split_vec) <- lambda_grid
## Get the value of lambda that corresponds to the smallest risk
lambda_opt <- lambda_grid[which.min(r_cv_split_vec)]
if (is.na(lambda_opt_param) == F) {
lambda_opt <- lambda_opt_param
}
## 2D grid search for gamma and theta (1D if user specifies there is no bias)
if (bias_indicator == F) {
theta_grid <- 0
}
## Iterate over every value of lambda and theta
gamma_theta_matrix <- matrix(NA, ncol=length(theta_grid), nrow=length(gamma_grid))
colnames(gamma_theta_matrix) <- theta_grid
rownames(gamma_theta_matrix) <- gamma_grid
for (i in 1:length(gamma_grid)) {
g <- gamma_grid[i]
for (j in 1:length(theta_grid)) {
t <- theta_grid[j]
score <- tweed.adj.fcn(lambda_opt, g, t, probs$p.tilde, probs$p_flip, pt,
probs, omega, basis_0, basis_1, basis_sum, basis_0.grid,
basis_1.grid, win_index, lose_index)
gamma_theta_matrix[i,j] <- score
}
}
## Get the value of gamma and theta that maximized the MLE
max_pair <- which(gamma_theta_matrix == max(gamma_theta_matrix), arr.ind = TRUE)
gamma_opt <- gamma_grid[max_pair[1]]
if (is.na(gamma_opt_param) == F) {
gamma_opt <- gamma_opt_param
}
theta_opt <- theta_grid[max_pair[2]]
###################
#### JS Method ####
###################
gridsearch.js <- sapply(const_grid, function(c) js_grid_fcn(c, probs$p.tilde, probs$p_flip, win_index, lose_index))
## Pick the value of C for each method
const_min <- const_grid[which.max(gridsearch.js)]
##################################
#### Adjust the probabilities ####
##################################
#### Create our adjusted probability estimates
p_capre <- tweedie_est(lambda_opt, gamma_opt, theta_opt, probs$p.tilde, probs$p_flip, pt, probs,
omega, basis_0, basis_1, basis_sum, basis_0.grid, basis_1.grid)
## Add in the naive estimate to our adj_prob dataframe
prob_adj <- cbind.data.frame(p.tilde=probs$p.tilde, p_capre=p_capre, probs$win_var, orig_order=probs$orig_order)
prob_adj <- prob_adj[order(prob_adj$orig_order), ]
#
# ## Items to return
items <- list(p_capre = prob_adj$p_capre,
probs = prob_adj,
gamma_opt = gamma_opt,
lambda_opt = lambda_opt,
theta_opt = theta_opt)
return(items)
}
########################
#### Here we will generate the plot
## Plt fcn
gen_empirical <- function(p.tilde, p.tilde2, ll, lu, ul, uu, win_var, win_var2, win_id, bias_indicator = F) {
## Get the user defined p.tilde and win variable
prob_df <- list(cbind.data.frame(p.tilde=p.tilde, home_win = win_var),
cbind.data.frame(p.tilde=p.tilde2, home_win=win_var2))
## Get the CAPRE estimate and win variable
tweedie_adj_probs <- list(cbind.data.frame(p.tilde=p.tilde, home_win = win_var),
cbind.data.frame(p.tilde=p.tilde2, home_win=win_var2))
## No split, No Bootstrapping
violation_naive <- avg_violation(prob_df, ll, lu, ul, uu, split_indicator = F, boot_indicator = F, bias_indicator = F)
## Yes split, No Bootstrapping
violation_tweedie <- avg_violation(tweedie_adj_probs, ll, lu, ul, uu, split_indicator = T, boot_indicator = F, bias_indicator = F)
## No split, Yes Bootstrapping
violation_naive_ci <- avg_violation(prob_df, ll, lu, ul, uu, split_indicator = F, boot_indicator = T, bias_indicator = F)
## Yes split, Yes Bootstrapping
violation_tweedie_ci <- avg_violation(tweedie_adj_probs, ll, lu, ul, uu, split_indicator = T, boot_indicator = T, bias_indicator = F)
obs_dta_naive <- violation_naive$Z
obs_dta_tweedie <- violation_tweedie$Z
obs_dta_naive_ci <- violation_naive_ci$Z
obs_dta_tweedie_ci <- violation_tweedie_ci$Z
items <- cbind.data.frame(obs_dta_naive, obs_dta_tweedie, obs_dta_naive_ci, obs_dta_tweedie_ci, naive_pbar=violation_naive$p_bar,
naive_C=violation_naive$C, n_delta=violation_naive$n_delta)
return(items)
}
avg_violation <- function(prob_df, ll, lu, ul, uu, split_indicator=T, boot_indicator=T, bias_indicator=F) {
## This one is the 2016
prob_df1 <- prob_df[[1]]
bucket_index_low <- which(prob_df1$p.tilde >= ll & prob_df1$p.tilde <= lu)
bucket_index_high <- which(prob_df1$p.tilde >= ul & prob_df1$p.tilde <= uu)
## This one is the 2017
prob_df2 <- prob_df[[2]]
bucket_index2_low <- which(prob_df2$p.tilde >= ll & prob_df2$p.tilde <= lu)
bucket_index2_high <- which(prob_df2$p.tilde >= ul & prob_df2$p.tilde <= uu)
if (boot_indicator==T) {
## Bootstrapping
# Index 1
bucket_index_low_boot <- sample(bucket_index_low, length(bucket_index_low), replace = T)
bucket_index_high_boot <-sample(bucket_index_high, length(bucket_index_high), replace = T)
# Index 2
bucket_index2_low_boot <- sample(bucket_index2_low, length(bucket_index2_low), replace = T)
bucket_index2_high_boot <-sample(bucket_index2_high, length(bucket_index2_high), replace = T)
prob_df_all <- rbind.data.frame(prob_df1[unique(union(bucket_index_low_boot, bucket_index_high_boot)),],
prob_df2[unique(union(bucket_index2_low_boot, bucket_index2_high_boot)),])
}
else if (boot_indicator==F) {
# Index 1
bucket_index_low_boot <- bucket_index_low
bucket_index_high_boot <- bucket_index_high
# Index 2
bucket_index2_low_boot <- bucket_index2_low
bucket_index2_high_boot <- bucket_index2_high
prob_df_all <- rbind.data.frame(prob_df1[unique(union(bucket_index_low_boot, bucket_index_high_boot)),],
prob_df2[unique(union(bucket_index2_low_boot, bucket_index2_high_boot)),])
}
if (split_indicator == T) {
rep_its <- replicate(100, {
p_tilde <- prob_df1$p.tilde
win_var <- prob_df1$home_win
## Half the games to get the tuning parameters
param_rows <- sample(1:length(p_tilde), floor(0.5*length(p_tilde)))
est_rows <- (1:length(p_tilde))[-param_rows]
rand_p <- p_tilde[param_rows]
win_var_rand <- win_var[param_rows]
est_p <- p_tilde[est_rows]
win_var_est <- win_var[est_rows]
adjust_info <- tweedie_adj(rand_p, win_var_rand, 1, bias_indicator = bias_indicator,
gamma_opt_param = NA, lambda_opt_param = NA)
gamma_param <- adjust_info$gamma_opt
lambda_param <- adjust_info$lambda_opt
ecap <- tweedie_adj(est_p, win_var_est, 1, bias_indicator = bias_indicator,
lambda_opt_param = lambda_param, gamma_opt_param = gamma_param)[[1]]
bucket_index_low_boot <- which(est_p >= ll & est_p <= lu)
bucket_index_high_boot <- which(est_p >= ul & est_p <= uu)
####################################
## This vector is the wrong size!!!
####################################
## Take in probs and return Z and C
## Average of probs less than lower thresh
lower_avg <- mean(ecap[bucket_index_low_boot])
## Average of probs greater than upper thresh
upper_avg <- mean(ecap[bucket_index_high_boot])
## Get table of extreme probs
summary_dta1 <- prob_df_all[which(prob_df_all$p.tilde >= ul & prob_df_all$p.tilde <= uu),]
tbl_up <- table(factor(summary_dta1$home_win, levels = 0:1))
summary_dta2 <- prob_df_all[which(prob_df_all$p.tilde >= ll & prob_df_all$p.tilde <= lu),]
tbl_low <- table(factor(summary_dta2$home_win, levels = 0:1))
## Compute observed probs & Conf Int
## Conf Int ##
n_delta <- length(bucket_index_low) + length(bucket_index_high) +
length(bucket_index2_low) + length(bucket_index2_high)
n_x <- sum(tbl_up)
n_y <- sum(tbl_low)
#p_x <- tbl_up[1] / n_x
#p_y <- tbl_low[2] / n_y
## C is the expected number of wins and losses
C <- (lower_avg*length(bucket_index_low_boot) +(1-upper_avg)*length(bucket_index_high_boot)) /
(length(bucket_index_high_boot)+length(bucket_index_low_boot))
## Z is EC Hat
Z <- ((tbl_up[1]+tbl_low[2]) / (n_x+n_y) - C) / C
Z_var <- (1-C) / (C*(n_x+n_y))
Z_sd <- sqrt(Z_var)
p_bar <- ((tbl_up[1]+tbl_low[2]) / (n_x+n_y))
return(c(C=C, Z=Z, Z_sd=Z_sd, n_x=n_x, n_y=n_y, p_bar=p_bar, n_delta=n_delta))
})
avg_reps <- rowMeans(rep_its)
C <- avg_reps[1]
Z <- avg_reps[2]
Z_sd <- avg_reps[3]
n_x <- avg_reps[4]
n_y <- avg_reps[5]
p_bar <- avg_reps[6]
n_delta <- avg_reps[7]
}
else if (split_indicator == F) {
## Take in probs and return Z and C
## Average of probs less than lower thresh
lower_avg <- mean(prob_df1$p.tilde[bucket_index_low_boot])
## Average of probs greater than upper thresh
upper_avg <- mean(prob_df1$p.tilde[bucket_index_high_boot])
## Get table of extreme probs
summary_dta1 <- prob_df_all[which(prob_df_all$p.tilde >= ul & prob_df_all$p.tilde <= uu),]
tbl_up <- table(factor(summary_dta1$home_win, levels = 0:1))
summary_dta2 <- prob_df_all[which(prob_df_all$p.tilde >= ll & prob_df_all$p.tilde <= lu),]
tbl_low <- table(factor(summary_dta2$home_win, levels = 0:1))
## Compute observed probs & Conf Int
## Conf Int ##
n_delta <- length(bucket_index_low) + length(bucket_index_high) +
length(bucket_index2_low) + length(bucket_index2_high)
n_x <- sum(tbl_up)
n_y <- sum(tbl_low)
#p_x <- tbl_up[1] / n_x
#p_y <- tbl_low[2] / n_y
## C is the expected number of wins and losses
C <- (lower_avg*length(bucket_index_low_boot) +(1-upper_avg)*length(bucket_index_high_boot)) /
(length(bucket_index_high_boot)+length(bucket_index_low_boot))
## Z is EC Hat
Z <- ((tbl_up[1]+tbl_low[2]) / (n_x+n_y) - C) / C
Z_var <- (1-C) / (C*(n_x+n_y))
Z_sd <- sqrt(Z_var)
p_bar <- ((tbl_up[1]+tbl_low[2]) / (n_x+n_y))
}
info <- list(C=C, Z=Z, Z_sd=Z_sd, n_x=n_x, n_y=n_y, p_bar=p_bar, n_delta=n_delta)
return(info)
}
boot_reps_storage
lapply(boot_reps_storage, function(l) mean(l[1,]))
unlist(lapply(boot_reps_storage, function(l) mean(l[1,])))
obs_dta_naive <- unlist(lapply(boot_reps_storage, function(l) mean(l[1,])))
obs_dta_tweedie <- unlist(lapply(boot_reps_storage, function(l) mean(l[2,])))
conf_int_low_naive <- unlist(lapply(boot_reps_storage, function(l) quantile(l[3,], c(0.05))))
conf_int_high_naive <- unlist(lapply(boot_reps_storage, function(l) quantile(l[3,], c(0.95))))
conf_int_low_tweedie <- unlist(lapply(boot_reps_storage, function(l) quantile(l[4,], c(0.05))))
conf_int_high_tweedie <- unlist(lapply(boot_reps_storage, function(l) quantile(l[4,], c(0.95))))
obs_dta_naive
obs_dta_tweedie
conf_int_low_naive
conf_int_high_naive
conf_int_low_tweedie
conf_int_high_tweedie
plt_vec_naive <- as.vector(obs_dta_naive)
plt_vec_tweedie <- as.vector(obs_dta_tweedie)
## Convert the time
time_convert_const <- vector()
time_add <- 15*(4-quarter_seq)
jj <- 1
for (qq in quarter_seq) {
time_convert_const <- c(time_convert_const, rep(time_add[jj], length(time_seq)))
jj <- jj + 1
}
time_left_orig <- as.numeric(rep(gsub(":",".", time_seq), length(quarter_seq))) + time_convert_const
temp_time <- strsplit(as.character(time_left_orig), "\\.")
time_left <- unlist(lapply(temp_time, function(i) {
if(length(i) > 1) {
dec <- i[2]
ifelse(length(dec) < 2, dec <- paste(dec,0,sep = ""), dec <- dec)
dec_conv <- unlist(strsplit(as.character(as.numeric(dec)/60), "\\."))[2]
}
else {
dec_conv <- 0
}
new_time <- as.numeric(paste(i[1], dec_conv, sep = "."))
return(new_time)
}))
## Add the naive line info w/ CI
conf_int_low_naive_save <- conf_int_low_naive
expected_line <- cbind.data.frame(time_order=1:length(plt_vec_naive),
avg_naive=plt_vec_naive, avg_tweedie=plt_vec_tweedie)
conf_int_low_naive <- sapply(conf_int_low_naive, function(p) {
if(p<=-1) {
p <- p+rnorm(1, mean=-2, sd=0.0001)
}
return(p)
})
expected_line$conf_up_naive <- as.vector(conf_int_high_naive)
expected_line$conf_low_naive <- as.vector(conf_int_low_naive)
## Add the tweedie line info w/ CI
conf_int_low_tweedie_save <- conf_int_low_tweedie
expected_line$conf_up_tweedie <- as.vector(conf_int_high_tweedie)
expected_line$conf_low_tweedie <- as.vector(conf_int_low_tweedie)
expected_line$time_left <- time_left
expected_line$time_left_orig <- time_left_orig
## Add the reverse of time left to make sense for the plot
expected_line$rev_time_left <- 60-expected_line$time_left
## Change expected_line to long instead of wide
expected_line_long <- expected_line %>%
gather(v, value, conf_up_naive:conf_low_tweedie) %>%
select(-one_of(c("time_order"))) %>%
separate(v, c("conf", "type"), sep = "conf_", extra = "merge") %>%
drop_na() %>%
spread(conf, type) %>%
rename(type = V1) %>%
separate(type, into = c("conf","type"),sep = "_")%>%
# bring upper and lower back into one row
spread(conf,value) %>%
# now we do the same with the avg values
gather(v,value, avg_naive:avg_tweedie) %>%
rename(adj_type=v) %>%
mutate(adj_type = substr(adj_type,5,nchar(adj_type))) %>%
filter(type == adj_type)
# expected_line_longrename(expected_line_long, "Adjustment Type" = adj_type)
## Grid for the plot object
expected_line_long$low[expected_line_long$low <=-1] <- -1
## Change levels for type
expected_line_long$adj_type[expected_line_long$adj_type=="naive"] <- "ESPN Unadjusted"
expected_line_long$adj_type[expected_line_long$adj_type=="tweedie"] <- "ECAP"
plt <- ggplot(expected_line_long, aes(x=time_left,
group=adj_type, colour=adj_type)) +
geom_line(size=1.5, aes(y=value)) +
scale_x_reverse(breaks=time_left, labels=time_seq) +
geom_line(aes(y=low), linetype="longdash", alpha=0.9, size=0.4) +
geom_line(aes(y=up), linetype="longdash", alpha=0.9, size=0.4) +
xlab("Seconds left in game") +
ylab("$\\widehat{EC_\\delta}$") +
theme_classic(base_size = 10) +
labs(color = "Method Type") +
theme(legend.position=c(0.2, 0.89), legend.title = element_text(size = 9), legend.text = element_text(size = 9),
axis.title.x = element_text(size=8), axis.title.y = element_text(size=8)) +
scale_color_manual(values=c("#009E73","#D55E00")) +
geom_hline(yintercept=0, linetype="dotted", size=0.8,
color = "black", alpha=0.82)
## loop over time stamps and quarters
## Time is of the form "minutes:seconds"3
time_seq <- c("01:00", "00:50", "00:40", "00:30", "00:20", "00:10")
quarter_seq <- 4 ## Note that some games have more than 4 quarters
plt_vec_naive <- as.vector(obs_dta_naive)
plt_vec_tweedie <- as.vector(obs_dta_tweedie)
## Convert the time
time_convert_const <- vector()
time_add <- 15*(4-quarter_seq)
jj <- 1
for (qq in quarter_seq) {
time_convert_const <- c(time_convert_const, rep(time_add[jj], length(time_seq)))
jj <- jj + 1
}
time_left_orig <- as.numeric(rep(gsub(":",".", time_seq), length(quarter_seq))) + time_convert_const
temp_time <- strsplit(as.character(time_left_orig), "\\.")
time_left <- unlist(lapply(temp_time, function(i) {
if(length(i) > 1) {
dec <- i[2]
ifelse(length(dec) < 2, dec <- paste(dec,0,sep = ""), dec <- dec)
dec_conv <- unlist(strsplit(as.character(as.numeric(dec)/60), "\\."))[2]
}
else {
dec_conv <- 0
}
new_time <- as.numeric(paste(i[1], dec_conv, sep = "."))
return(new_time)
}))
## Add the naive line info w/ CI
conf_int_low_naive_save <- conf_int_low_naive
expected_line <- cbind.data.frame(time_order=1:length(plt_vec_naive),
avg_naive=plt_vec_naive, avg_tweedie=plt_vec_tweedie)
conf_int_low_naive <- sapply(conf_int_low_naive, function(p) {
if(p<=-1) {
p <- p+rnorm(1, mean=-2, sd=0.0001)
}
return(p)
})
expected_line$conf_up_naive <- as.vector(conf_int_high_naive)
expected_line$conf_low_naive <- as.vector(conf_int_low_naive)
## Add the tweedie line info w/ CI
conf_int_low_tweedie_save <- conf_int_low_tweedie
expected_line$conf_up_tweedie <- as.vector(conf_int_high_tweedie)
expected_line$conf_low_tweedie <- as.vector(conf_int_low_tweedie)
expected_line$time_left <- time_left
expected_line$time_left_orig <- time_left_orig
## Add the reverse of time left to make sense for the plot
expected_line$rev_time_left <- 60-expected_line$time_left
## Change expected_line to long instead of wide
expected_line_long <- expected_line %>%
gather(v, value, conf_up_naive:conf_low_tweedie) %>%
select(-one_of(c("time_order"))) %>%
separate(v, c("conf", "type"), sep = "conf_", extra = "merge") %>%
drop_na() %>%
spread(conf, type) %>%
rename(type = V1) %>%
separate(type, into = c("conf","type"),sep = "_")%>%
# bring upper and lower back into one row
spread(conf,value) %>%
# now we do the same with the avg values
gather(v,value, avg_naive:avg_tweedie) %>%
rename(adj_type=v) %>%
mutate(adj_type = substr(adj_type,5,nchar(adj_type))) %>%
filter(type == adj_type)
# expected_line_longrename(expected_line_long, "Adjustment Type" = adj_type)
## Grid for the plot object
expected_line_long$low[expected_line_long$low <=-1] <- -1
## Change levels for type
expected_line_long$adj_type[expected_line_long$adj_type=="naive"] <- "ESPN Unadjusted"
expected_line_long$adj_type[expected_line_long$adj_type=="tweedie"] <- "ECAP"
plt <- ggplot(expected_line_long, aes(x=time_left,
group=adj_type, colour=adj_type)) +
geom_line(size=1.5, aes(y=value)) +
scale_x_reverse(breaks=time_left, labels=time_seq) +
geom_line(aes(y=low), linetype="longdash", alpha=0.9, size=0.4) +
geom_line(aes(y=up), linetype="longdash", alpha=0.9, size=0.4) +
xlab("Seconds left in game") +
ylab("$\\widehat{EC_\\delta}$") +
theme_classic(base_size = 10) +
labs(color = "Method Type") +
theme(legend.position=c(0.2, 0.89), legend.title = element_text(size = 9), legend.text = element_text(size = 9),
axis.title.x = element_text(size=8), axis.title.y = element_text(size=8)) +
scale_color_manual(values=c("#009E73","#D55E00")) +
geom_hline(yintercept=0, linetype="dotted", size=0.8,
color = "black", alpha=0.82)
plt
plt <- ggplot(expected_line_long, aes(x=time_left,
group=adj_type, colour=adj_type)) +
geom_line(size=1.5, aes(y=value)) +
scale_x_reverse(breaks=time_left, labels=time_seq) +
geom_line(aes(y=low), linetype="longdash", alpha=0.9, size=0.4) +
geom_line(aes(y=up), linetype="longdash", alpha=0.9, size=0.4) +
xlab("Seconds left in game") +
ylab("$\\widehat{EC_\\delta}$") +
theme_classic(base_size = 10) +
labs(color = "Method Type") +
theme(legend.position=c(0.2, 0.89), legend.title = element_text(size = 9), legend.text = element_text(size = 9),
axis.title.x = element_text(size=8), axis.title.y = element_text(size=8)) +
scale_color_manual(values=c("white","#D55E00")) +
geom_hline(yintercept=0, linetype="dotted", size=0.8,
color = "black", alpha=0.82)
plt
## Tikz code
tikz(file = "ESPN PhD Seminar Start", width = 5.5, height = 2.8)
plt
dev.off()
require(lubridate)
require(tidyverse)
require(tidyr)
require(progress)
require(splines)
require(quadprog)
require(ggplot2)
library(tikzDevice)
library(parallel)
## Tikz code
tikz(file = "ESPN PhD Seminar Start", width = 5.5, height = 2.8)
plt
dev.off()
plt <- ggplot(expected_line_long, aes(x=time_left,
group=adj_type, colour=adj_type)) +
geom_line(size=1.5, aes(y=value)) +
scale_x_reverse(breaks=time_left, labels=time_seq) +
geom_line(aes(y=low), linetype="longdash", alpha=0.9, size=0.4) +
geom_line(aes(y=up), linetype="longdash", alpha=0.9, size=0.4) +
xlab("Seconds left in game") +
ylab("$\\widehat{EC_\\delta}$") +
theme_classic(base_size = 10) +
labs(color = "Method Type") +
theme(legend.position=c(0.2, 0.89), legend.title = element_text(size = 9), legend.text = element_text(size = 9),
axis.title.x = element_text(size=8), axis.title.y = element_text(size=8)) +
scale_color_manual(values=c("#009E73","#D55E00")) +
geom_hline(yintercept=0, linetype="dotted", size=0.8,
color = "black", alpha=0.82)
## Tikz code
tikz(file = "ESPN PhD Seminar End", width = 5.5, height = 2.8)
plt
dev.off()
## Tikz code
tikz(file = "end.tex", width = 5.5, height = 2.8)
plt
dev.off()
plt <- ggplot(expected_line_long, aes(x=time_left,
group=adj_type, colour=adj_type)) +
geom_line(size=1.5, aes(y=value)) +
scale_x_reverse(breaks=time_left, labels=time_seq) +
geom_line(aes(y=low), linetype="longdash", alpha=0.9, size=0.4) +
geom_line(aes(y=up), linetype="longdash", alpha=0.9, size=0.4) +
xlab("Seconds left in game") +
ylab("$\\widehat{EC_\\delta}$") +
theme_classic(base_size = 10) +
labs(color = "Method Type") +
theme(legend.position=c(0.2, 0.89), legend.title = element_text(size = 9), legend.text = element_text(size = 9),
axis.title.x = element_text(size=8), axis.title.y = element_text(size=8)) +
scale_color_manual(values=c("white","#D55E00")) +
geom_hline(yintercept=0, linetype="dotted", size=0.8,
color = "black", alpha=0.82)
## Tikz code
tikz(file = "start.tex", width = 5.5, height = 2.8)
plt
dev.off()
lm(rnorm(1000) ~ rnorm(1000), data=cars)
lm_model <- lm(rnorm(1000) ~ rnorm(1000), data=cars)
lm_model
summary(lm_model)
typeof(lm_mp)
typeof(lm_model)
lm_model[[1]]
str(lm_model)
